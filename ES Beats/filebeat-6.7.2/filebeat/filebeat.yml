# filebeat.yml

filebeat.inputs:
- type: log
  enabled: true
  paths:
    #- /var/log/*.log 
    #- '/usr/share/filebeat/dockerlogs/*/*.log'
    - '/var/lib/docker/containers/*/*.log'

- type: docker
  #enabled: false

  # Combine partial lines flagged by `json-file` format
  combine_partials: true

  # Use this to read from all containers, replace * with a container id to read from one:
  containers:
    #stream: all # can be all, stdout or stderr
    ids:
      - '*'

processors:
 - decode_json_fields:
     fields: ["message"]
     max_depth: 20
     target: "event"
     overwrite_keys: true
 

#filebeat.config.modules:
#  path: ${path.config}/modules.d/*.yml
#  reload.enabled: false


#setup.template.settings:
#  index.number_of_shards: 3
filebeat.registry.path: registry
filebeat.registry.file_permissions: 0600

path.home: /usr/share/filebeat/
path.data: /usr/share/filebeat/data
path.logs: /usr/share/filebeat/logs

setup.kibana:
# if your kibana is local you can change it to 127.0.0.1:80
  host: "localhost:5601"
  #protocol: "http"
# identification required for X-pack  
#  username: "my_login"
#  password: "my_password"
# path needed if kibana not at website root  
  #path: "/kibana"

#output.logstash:
  # The Logstash hosts
#  hosts: ["172.20.10.2:5044"]

output.elasticsearch:
# if your elasticsearch is local you can change it to 127.0.0.1:9200
  hosts: ["192.168.1.110:9200"]
  #template:
  #  name: "filebeat"
  #  path: "fields.yml"
  #  overwrite: false
  #protocol: "http"

# identification required for X-pack  
#  username: "my_login"
#  password: "my_password"


# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
#logging.to_files: true
#logging.to_syslog: false


#xpack.monitoring:
#  enabled: true
#  elasticsearch: 
    #hosts: ["172.20.10.2:9200"]

# X-pack optional module
#xpack.monitoring.enabled: true
#xpack.monitoring.elasticsearch: